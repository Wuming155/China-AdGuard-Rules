import requests
import re
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import urllib3

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚çš„è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®åŒº ---
SOURCES_FILE = 'sources.txt'
README_FILE = 'README.md'
DIST_DIR = 'dist'

# æ–‡ä»¶åä¸ README ä¸­æ˜¾ç¤ºåç§°çš„å¯¹åº”å…³ç³»
TITLE_MAP = {
    'hosts_rules.txt': 'Hosts å±è”½è§„åˆ™',
    'adguard_rules.txt': 'AdGuard è¿‡æ»¤è§„åˆ™',
    'whitelist.txt': 'ç™½åå•æ”¾è¡Œè§„åˆ™'
}

def get_file_header(filename, count):
    """ä¸ºç”Ÿæˆçš„è§„åˆ™æ–‡ä»¶æ·»åŠ å¤´éƒ¨ä¿¡æ¯"""
    date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    display_name = TITLE_MAP.get(filename, filename.replace('.txt', ''))
    return f"# æ›´æ–°æ—¥æœŸï¼š{date_str}\n# è§„åˆ™æ•°ï¼š{count}\n! Title: {display_name}\n! ------------------------------------\n\n"

def fetch_url(url):
    """æŠ“å– URL å†…å®¹"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        r = requests.get(url, headers=headers, timeout=30, verify=False)
        if r.status_code == 200:
            return r.text.splitlines()
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥ {url}: {e}")
    return []

def update_readme(file_stats):
    """å½»åº•ä¿®å¤é‡å¤è¿½åŠ é—®é¢˜çš„æ›´æ–°å‡½æ•°"""
    readme_path = 'README.md'
    if not os.path.exists(readme_path): 
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° README.md")
        return

    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # 1. æ„é€ æ–°çš„è¡¨æ ¼å†…å®¹
    table_rows = ""
    for filename in sorted(file_stats.keys()):
        count = file_stats[filename]
        display_name = TITLE_MAP.get(filename, filename.replace('.txt', ''))
        table_rows += f"| **{display_name}** | {count} | [ç‚¹å‡»ä¸‹è½½](./dist/{filename}) |\n"

    date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # æ³¨æ„ï¼šè¿™é‡Œçš„æ ‡è®°ä½å¿…é¡»ä¿æŒè¿™ä¸€è¡Œçš„çº¯å‡€
    new_block = f"\n### ğŸ“Š è§„åˆ™ç»Ÿè®¡\n| è§„åˆ™ç±»å‹ | è§„åˆ™æ•°é‡ | ä¸‹è½½é“¾æ¥ |\n| :--- | :--- | :--- |\n{table_rows}\n**â° æœ€åæ›´æ–°æ—¶é—´**: {date_str}\n"

    # 2. ä½¿ç”¨æ­£åˆ™åŒ¹é…ã€‚æ ¸å¿ƒé€»è¾‘ï¼šåŒ¹é…ä» åˆ° çš„æ‰€æœ‰å†…å®¹
    # ä¿®å¤ï¼šé˜²æ­¢å› ä¸ºæ¢è¡Œç¬¦ä¸åŒå¯¼è‡´çš„åŒ¹é…å¤±è´¥
    pattern = re.compile(r'.*?', re.DOTALL)

    if pattern.search(content):
        # å¦‚æœæ‰¾åˆ°äº†æ ‡è®°ä½ï¼Œç›´æ¥ç²¾å‡†æ›¿æ¢
        updated_content = pattern.sub(new_block, content)
        print("å‘ç°æ ‡è®°ä½ï¼Œæ‰§è¡Œç²¾å‡†æ›¿æ¢ã€‚")
    else:
        # å¦‚æœæ‰¾ä¸åˆ°æ ‡è®°ä½ï¼Œè¯´æ˜ä½ çš„ README é‡Œæ ‡è®°å†™é”™äº†æˆ–æ²¡äº†
        # ä¸ºäº†é˜²æ­¢ç»§ç»­æ— é™è¿½åŠ ï¼Œæˆ‘ä»¬æŠ¥é”™å¹¶æç¤ºä½ æ‰‹åŠ¨æ£€æŸ¥
        print("ï¼ï¼ï¼è‡´å‘½é”™è¯¯ï¼šåœ¨ README.md ä¸­æ²¡æ‰¾åˆ°åŒ¹é…çš„æ ‡è®°ä½ ï¼ï¼ï¼")
        print("è¯·æ£€æŸ¥ README.md æ˜¯å¦åŒ…å«å®Œæ•´çš„ å’Œ ")
        return

    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(updated_content)

def run():
    # è§„åˆ™åˆ†ç±»å®¹å™¨
    collections = {
        'hosts_rules.txt': set(),
        'adguard_rules.txt': set(),
        'whitelist.txt': set()
    }

    if not os.path.exists(SOURCES_FILE):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ° {SOURCES_FILE}")
        return
        
    with open(SOURCES_FILE, 'r', encoding='utf-8') as f:
        urls = re.findall(r'https?://[^\s\]]+', f.read())

    print(f"å¼€å§‹æŠ“å– {len(urls)} ä¸ªæº...")
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_url, urls)

    for lines in results:
        for line in lines:
            line = line.strip()
            # æ’é™¤ç©ºè¡Œå’Œç®€å•çš„æ³¨é‡Šï¼ˆ! æˆ– #ç©ºæ ¼ï¼‰ï¼Œä½†ä¿ç•™ ### è§„åˆ™
            if not line or line.startswith('!') or line.startswith('# '):
                continue
            
            # 1. åˆ¤å®šç™½åå•
            if line.startswith('@@'):
                collections['whitelist.txt'].add(line)
            # 2. åˆ¤å®š Hosts æ ¼å¼
            elif line.startswith('0.0.0.0') or line.startswith('127.0.0.1'):
                parts = line.split()
                if len(parts) >= 2:
                    # ç»Ÿä¸€è½¬æˆ 0.0.0.0 å¹¶æå–åŸŸå
                    domain = parts[1]
                    collections['hosts_rules.txt'].add(f"0.0.0.0 {domain}")
            # 3. å‰©ä¸‹çš„å…¨æ”¾è¿› AdGuard è§„åˆ™
            else:
                collections['adguard_rules.txt'].add(line)

    # å¤„ç†ä¿å­˜é€»è¾‘
    os.makedirs(DIST_DIR, exist_ok=True)
    file_stats = {}

    for filename, rules in collections.items():
        if rules:  # åªæœ‰å½“è¯¥åˆ†ç±»æœ‰è§„åˆ™æ—¶æ‰åˆ›å»ºæ–‡ä»¶
            count = len(rules)
            file_stats[filename] = count
            file_path = os.path.join(DIST_DIR, filename)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(get_file_header(filename, count))
                f.write("\n".join(sorted(list(rules))))
            print(f"å·²ç”Ÿæˆ: {filename} (å…± {count} æ¡)")

    # æ›´æ–° README ç»Ÿè®¡
    update_readme(file_stats)

if __name__ == "__main__":
    run()
