import requests
import re
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®ï¼šæ–‡ä»¶åå¯¹åº”çš„æ˜¾ç¤ºåç§°
TITLE_MAP = {
    'hosts_rules.txt': 'Hosts å±è”½è§„åˆ™',
    'adguard_rules.txt': 'AdGuard è¿‡æ»¤è§„åˆ™',
    'whitelist.txt': 'ç™½åå•æ”¾è¡Œè§„åˆ™'
}

def get_file_header(filename, count):
    date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    display_name = TITLE_MAP.get(filename, filename.replace('.txt', ''))
    return f"# æ›´æ–°æ—¥æœŸï¼š{date_str}\n# è§„åˆ™æ•°ï¼š{count}\n! Title: {display_name}\n! ------------------------------------\n\n"

def fetch_url(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        r = requests.get(url, headers=headers, timeout=30, verify=False)
        if r.status_code == 200:
            return r.text.splitlines()
    except:
        pass
    return []

def update_live_readme(file_stats):
    """
    file_stats: æ ¼å¼ä¸º {'æ–‡ä»¶å.txt': æ•°é‡, ...}
    æ ¹æ®å®é™…ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡ï¼ŒåŠ¨æ€æ„å»ºè¡¨æ ¼
    """
    readme_path = 'README.md'
    if not os.path.exists(readme_path): return

    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ„å»ºè¡¨æ ¼è¡Œ
    table_rows = ""
    for filename, count in sorted(file_stats.items()):
        display_name = TITLE_MAP.get(filename, filename.replace('.txt', ''))
        table_rows += f"| **{display_name}** | {count} | [ç‚¹å‡»ä¸‹è½½](./dist/{filename}) |\n"

    date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    new_stats = f"""### ğŸ“Š è§„åˆ™ç»Ÿè®¡
| è§„åˆ™ç±»å‹ | è§„åˆ™æ•°é‡ | ä¸‹è½½é“¾æ¥ |
| :--- | :--- | :--- |
{table_rows}
**â° æœ€åæ›´æ–°æ—¶é—´**: {date_str}
"""

    pattern = re.compile(r'.*?', re.DOTALL)
    if pattern.search(content):
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(pattern.sub(new_stats, content))
        print("README ç»Ÿè®¡å·²æ ¹æ®å®é™…æ–‡ä»¶æ•°é‡è‡ªåŠ¨æ›´æ–°ã€‚")

def run():
    # ä½¿ç”¨å­—å…¸ï¼Œæ”¯æŒåŠ¨æ€å¢åŠ åˆ†ç±»
    collections = {
        'hosts_rules.txt': set(),
        'adguard_rules.txt': set(),
        'whitelist.txt': set()
    }

    if not os.path.exists('sources.txt'): return
    with open('sources.txt', 'r', encoding='utf-8') as f:
        urls = re.findall(r'https?://[^\s\]]+', f.read())

    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_url, urls)

    for lines in results:
        for line in lines:
            line = line.strip()
            if not line or line.startswith('!') or (line.startswith('#') and not line.startswith('##')):
                continue
            
            # 1. ç™½åå•åˆ¤å®š
            if line.startswith('@@'):
                collections['whitelist.txt'].add(line)
            # 2. Hosts åˆ¤å®š
            elif line.startswith('0.0.0.0') or line.startswith('127.0.0.1'):
                parts = line.split()
                if len(parts) >= 2:
                    collections['hosts_rules.txt'].add(f"0.0.0.0 {parts[1]}")
            # 3. å…¶ä»–æ‰€æœ‰è§„åˆ™ï¼ˆCSS, é€šé…ç¬¦ç­‰ï¼‰
            else:
                collections['adguard_rules.txt'].add(line)

    # è¿‡æ»¤æ‰ç©ºçš„åˆ†ç±»ï¼Œåªå¤„ç†æœ‰å†…å®¹çš„æ–‡ä»¶
    active_collections = {k: v for k, v in collections.items() if v}
    
    os.makedirs('dist', exist_ok=True)
    file_stats = {}

    for filename, rules in active_collections.items():
        count = len(rules)
        file_stats[filename] = count
        with open(f'dist/{filename}', 'w', encoding='utf-8') as f:
            f.write(get_file_header(filename, count))
            f.write("\n".join(sorted(list(rules))))

    # åŠ¨æ€ç»Ÿè®¡ï¼šç”Ÿæˆäº†å‡ ä¸ªæ–‡ä»¶ï¼ŒREADME å°±åˆ—å‡ºå‡ ä¸ª
    update_live_readme(file_stats)

if __name__ == "__main__":
    run()
