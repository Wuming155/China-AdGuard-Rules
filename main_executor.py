import requests
import re
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import urllib3

# ç¦ç”¨ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_file_header(name, count):
    """æ‰¾å›æ‚¨ä¸¢å¤±çš„æ ‡é¢˜ä¿¡æ¯"""
    # ä¸¥æ ¼æŒ‰ç…§ä½ è¦æ±‚çš„æ—¥æœŸæ ¼å¼
    date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    title_map = {
        'hosts_rules': 'Hosts å±è”½è§„åˆ™',
        'adguard_rules': 'AdGuard è¿‡æ»¤è§„åˆ™'
    }
    return f"# æ›´æ–°æ—¥æœŸï¼š{date_str}\n# è§„åˆ™æ•°ï¼š{count}\n! Title: {title_map.get(name, 'å»å¹¿å‘Šè§„åˆ™')}\n! ------------------------------------\n\n"

def fetch_url(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    try:
        r = requests.get(url, headers=headers, timeout=30, verify=False)
        if r.status_code == 200:
            return r.text.splitlines()
    except:
        pass
    return []

def update_live_readme(hosts_num, other_num):
    """åŠ¨æ€æ›´æ–° README ä¸­çš„ç»Ÿè®¡æ•°æ®"""
    readme_path = 'README.md'
    if not os.path.exists(readme_path):
        return

    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()

    date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    # åŒ¹é…æ ‡è®°ä½è¿›è¡Œæ›¿æ¢
    new_stats = f"""### ğŸ“Š è§„åˆ™ç»Ÿè®¡
| è§„åˆ™ç±»å‹ | è§„åˆ™æ•°é‡ | ä¸‹è½½é“¾æ¥ |
| :--- | :--- | :--- |
| **Hosts å±è”½** | {hosts_num} | [ç‚¹å‡»ä¸‹è½½](./dist/hosts_rules.txt) |
| **AdGuard è¿‡æ»¤** | {other_num} | [ç‚¹å‡»ä¸‹è½½](./dist/adguard_rules.txt) |

**â° æœ€åæ›´æ–°æ—¶é—´**: {date_str}
"""

    pattern = re.compile(r'.*?', re.DOTALL)
    if pattern.search(content):
        updated_content = pattern.sub(new_stats, content)
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)

def run():
    host_set = set()
    other_set = set()

    if not os.path.exists('sources.txt'):
        return
        
    with open('sources.txt', 'r', encoding='utf-8') as f:
        urls = re.findall(r'https?://[^\s\]]+', f.read())

    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_url, urls)

    for lines in results:
        for line in lines:
            line = line.strip()
            # æ’é™¤æ³¨é‡Šå’Œç©ºè¡Œï¼Œä½†ä¿ç•™åƒ ### è¿™æ ·çš„è§„åˆ™
            if not line or line.startswith('!') or line.startswith('# '):
                continue
            
            if line.startswith('0.0.0.0') or line.startswith('127.0.0.1'):
                parts = line.split()
                if len(parts) >= 2:
                    host_set.add(f"0.0.0.0 {parts[1]}")
            else:
                other_set.add(line)

    # ä¿å­˜æ–‡ä»¶
    os.makedirs('dist', exist_ok=True)
    
    # 1. ä¿å­˜ Hosts è§„åˆ™ï¼ˆåŒ…å«æ ‡é¢˜ï¼‰
    with open('dist/hosts_rules.txt', 'w', encoding='utf-8') as f:
        f.write(get_file_header('hosts_rules', len(host_set)))
        f.write("\n".join(sorted(list(host_set))))

    # 2. ä¿å­˜ AdGuard è§„åˆ™ï¼ˆåŒ…å«æ ‡é¢˜ï¼‰
    with open('dist/adguard_rules.txt', 'w', encoding='utf-8') as f:
        f.write(get_file_header('adguard_rules', len(other_set)))
        f.write("\n".join(sorted(list(other_set))))

    # 3. æ›´æ–° README
    update_live_readme(len(host_set), len(other_set))

    print(f"å¤„ç†å®Œæˆï¼šHosts({len(host_set)}æ¡), å…¶ä»–({len(other_set)}æ¡)")

if __name__ == "__main__":
    run()
